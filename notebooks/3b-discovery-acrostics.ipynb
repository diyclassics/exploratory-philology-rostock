{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "from natsort import natsorted\n",
    "import pickle\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus, get files\n",
    "T = LatinTesseraeCorpusReader()\n",
    "words = T.words()\n",
    "\n",
    "print(next(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for preprocessing\n",
    "def preprocess(text, lower=True, normalize=True, punctuation=False, \n",
    "                numbers=False, unhyphenate=False, remove_lines=False, \n",
    "                remove_spaces=False, entities=False, diacriticals=True, fill=' '):\n",
    "    \n",
    "    import html\n",
    "    import re\n",
    "    import unicodedata\n",
    "    from cltk.alphabet.lat import JVReplacer\n",
    "    replacer = JVReplacer()\n",
    "\n",
    "\n",
    "    if not entities:\n",
    "        text = html.unescape(text)\n",
    "\n",
    "    if unhyphenate:\n",
    "        text = re.sub(r'[-»—]\\s?\\n', '', text, flags=re.MULTILINE)    \n",
    "\n",
    "    if lower:\n",
    "        text = text.lower() # Lowercase\n",
    "\n",
    "    if normalize:\n",
    "        text = replacer.replace(text)\n",
    "\n",
    "    if not punctuation:\n",
    "        # Remove punctuation\n",
    "        punctuation =\"\\\"#$%&\\'()*+,/:;<=>@[\\]^_`{|}~.?!«»—“-”\"\n",
    "        misc = '¡£¤¥¦§¨©¯°±²³´µ¶·¸¹º¼½¾¿÷·–‘’†•ↄ∞⏑〈〉（）'\n",
    "        misc += punctuation\n",
    "        translator = str.maketrans({key: fill for key in misc})\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if not numbers:\n",
    "        # Remove numbers\n",
    "        translator = str.maketrans({key: fill for key in '0123456789'})\n",
    "        text = text.translate(translator)\n",
    "\n",
    "    if remove_lines:\n",
    "        text = \" \".join(text.split('\\n'))\n",
    "\n",
    "    if remove_spaces:\n",
    "        text = fill.join(text.split())\n",
    "    def remove_diacriticals(text):\n",
    "        combining_character_table = dict.fromkeys(c for c in range(sys.maxunicode) if unicodedata.combining(chr(c)))\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = text.translate(combining_character_table)\n",
    "        return text        \n",
    "\n",
    "    if not diacriticals:\n",
    "        text = remove_diacriticals(text)\n",
    "\n",
    "    # Fix spacing\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word list\n",
    "\n",
    "# preprocessed_texts = [preprocess(text) for text in T.texts()]\n",
    "# words_texts = [text.split() for text in preprocessed_texts]\n",
    "# wordlist = [item for subl in words_texts for item in subl]\n",
    "# wordlist = set(wordlist)\n",
    "# wordlist = sorted(wordlist)\n",
    "\n",
    "# with open('../data/verba.txt', 'w') as f:\n",
    "#     for word in wordlist:\n",
    "#         f.write(f'{word}\\n')\n",
    "\n",
    "# pickle.dump(wordlist, open('../data/verba.pickle', 'wb'))\n",
    "\n",
    "wordlist = pickle.load(open('../data/verba.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample\n",
    "\n",
    "ilias = 'italicus.ilias_latina.tess'\n",
    "ilias_text = next(T.texts(ilias))\n",
    "ilias_lines = ilias_text.split('\\n')\n",
    "pprint(ilias_lines[:8])\n",
    "print()\n",
    "pprint(ilias_lines[-8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO-CODE\n",
    "# - ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'scripsit' in wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a marginal text\n",
    "margin_list = [line[0] for line in ilias_lines]\n",
    "margin_list[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_text = ''.join(margin_list)\n",
    "margin_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_text = \"\".join(preprocess(margin_text, remove_spaces=True).split())\n",
    "margin_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ngrams\n",
    "\n",
    "from nltk import ngrams\n",
    "ilias_bigrams = list(ngrams(margin_text, 2))\n",
    "pprint(ilias_bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilias_bigrams = [''.join(bigram) for bigram in ilias_bigrams]\n",
    "pprint(ilias_bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in ilias_bigrams[:10]:\n",
    "    if bigram in wordlist:\n",
    "        print(f'WE FOUND AN ACROSTIC!!!!!!! {bigram}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 8grams\n",
    "\n",
    "ilias_ngrams = [''.join(ngram) for ngram in ngrams(margin_text, 8)]\n",
    "pprint(ilias_ngrams[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ilias_ngrams:\n",
    "    if ngram in wordlist:\n",
    "        print(f'WE FOUND AN ACROSTIC!!!!!!! {ngram}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize!\n",
    "\n",
    "def find_acrostics(text, wordlist, n=5, preprocess=None):\n",
    "    lines = text.split('\\n')\n",
    "    if preprocess:\n",
    "        lines = [preprocess(line) for line in lines]\n",
    "        lines = [line for line in lines if line]\n",
    "    acrostic_margin = ''.join([line[0] for line in lines])\n",
    "    acrostic_ngrams = [''.join(ngram) for ngram in ngrams(acrostic_margin, n)]\n",
    "\n",
    "    acrostic_matches = []\n",
    "\n",
    "    for acrostic_ngram in acrostic_ngrams:\n",
    "        if acrostic_ngram in wordlist:\n",
    "            acrostic_matches.append(acrostic_ngram)\n",
    "    \n",
    "    return acrostic_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_acrostics(ilias_text, wordlist, n=8, preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_acrostics(ilias_text, wordlist, n=16, preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_acrostics(ilias_text, wordlist, n=4, preprocess=preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6,12):\n",
    "    print(f'Acrostics of length {i}')\n",
    "    print(f'{find_acrostics(ilias_text, wordlist, n=i, preprocess=preprocess)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over Aeneid\n",
    "# NB: Cached, takes ~5 mins to run\n",
    "\n",
    "# aeneid = [file for file in T.fileids() if 'vergil.aeneid' in file]\n",
    "# aeneid = natsorted(aeneid)\n",
    "# aeneid_texts = [next(T.texts(book)) for book in aeneid]\n",
    "\n",
    "# aeneid_acrostics = []\n",
    "\n",
    "# for text in aeneid_texts:\n",
    "#     for i in range(4,12):\n",
    "#         matches = find_acrostics(text, wordlist, n=i, preprocess=preprocess)\n",
    "#         aeneid_acrostics.extend(matches)\n",
    "\n",
    "# pickle.dump(aeneid_acrostics, open('../data/acrostics.pickle', 'wb'))\n",
    "\n",
    "aeneid_acrostics = pickle.load(open('../data/acrostics.pickle', 'rb'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(random.sample(aeneid_acrostics, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aeneid_acrostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "aeneid_acrostic_lens = defaultdict(list)\n",
    "\n",
    "for acrostic in aeneid_acrostics:\n",
    "    aeneid_acrostic_lens[len(acrostic)].append(acrostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeneid_acrostic_lens[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs, ys = zip(*aeneid_acrostic_lens.items())\n",
    "xs = [str(x) for x in xs]\n",
    "ys = [len(y) for y in ys]\n",
    "   \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plt.bar(xs, ys, color ='maroon', width = 0.4)\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Matches\")\n",
    "plt.title(\"Acrostic lengths in the Aeneid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "acrostic_counter = Counter(aeneid_acrostics)\n",
    "pprint(acrostic_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For free experiments...\n",
    "# - Refactor for another author?\n",
    "# - How could you refactor for telestichs? Mesostichs? Gamma acrostics?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('stanford')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f28f2655caa070e39b75c186f98b8f52da1af34bdb8dab0b58a93e2439370a2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
