{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new reader\n",
    "\n",
    "class VicifonsCorpusReader(PlaintextCorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for working with Vicifons-sourced plaintext docs\n",
    "    \"\"\"\n",
    "    def __init__(self, root, fileids):\n",
    "        PlaintextCorpusReader.__init__(self, root, fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate reader \n",
    "\n",
    "root = 'texts/vicifons'\n",
    "file_pattern = '.+\\.txt'\n",
    "CR = VicifonsCorpusReader(root, file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show fileids\n",
    " \n",
    "CR.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default sents\n",
    "\n",
    "CR.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default words\n",
    "\n",
    "CR.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default raw\n",
    "\n",
    "print(CR.raw()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default raw, pretty printed\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(CR.raw()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No lines method, force error\n",
    "\n",
    "CR.lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new reader, with new lines method\n",
    "\n",
    "class VicifonsCorpusReader(PlaintextCorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for working with Vicifons-sourced plaintext docs\n",
    "    \"\"\"\n",
    "    def __init__(self, root, fileids):\n",
    "        PlaintextCorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def lines(self, fileids=None):\n",
    "        raw = self.raw()\n",
    "        lines = raw.split('\\n')\n",
    "        for line in lines:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate reader, get lines, print lines\n",
    "\n",
    "root = 'texts/vicifons'\n",
    "file_pattern = '.+\\.txt'\n",
    "CR = VicifonsCorpusReader(root, file_pattern)\n",
    "\n",
    "lines = CR.lines()\n",
    "\n",
    "print(next(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more lines\n",
    "\n",
    "print(next(lines))\n",
    "print(next(lines))\n",
    "print(next(lines))\n",
    "print(next(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new sents method, i.e. override\n",
    "\n",
    "from cltk.sentence.lat import LatinPunktSentenceTokenizer\n",
    "sent_tokenizer= LatinPunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new reader, with new sents method\n",
    "\n",
    "class VicifonsCorpusReader(PlaintextCorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for working with Vicifons-sourced plaintext docs\n",
    "    \"\"\"\n",
    "    def __init__(self, root, fileids):\n",
    "        PlaintextCorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def lines(self, fileids=None):\n",
    "        raw = self.raw(fileids)\n",
    "        lines = raw.split('\\n')\n",
    "        for line in lines:\n",
    "            yield line\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        raw = self.raw(fileids)\n",
    "        raw = \" \".join(raw.split('\\n'))\n",
    "        sentences = sent_tokenizer.tokenize(raw)\n",
    "        for sent in sentences:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate reader, get sents, print sents\n",
    "\n",
    "root = 'texts/vicifons'\n",
    "file_pattern = '.+\\.txt'\n",
    "CR = VicifonsCorpusReader(root, file_pattern)\n",
    "\n",
    "sents = CR.sents()\n",
    "\n",
    "print(next(sents))\n",
    "print(next(sents))\n",
    "print(next(sents))\n",
    "print(next(sents))\n",
    "print(next(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why use custom reader/custom methods? Latin sentence segnmentation example\n",
    "\n",
    "# Cic. Cat 1.7\n",
    "test = \"Meministine me ante diem XII Kalendas Novembris dicere in senatu fore in armis certo die, qui dies futurus esset ante diem VI Kal. Novembris, C. Manlium, audaciae satellitem atque administrum tuae?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence segmentation with NLTK\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk_sents = sent_tokenize(test)\n",
    "\n",
    "for i, sent in enumerate(nltk_sents, 1):\n",
    "    print(f'{i}: {sent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence segmentation with CLTK\n",
    "\n",
    "from cltk.sentence.lat import LatinPunktSentenceTokenizer\n",
    "sent_tokenizer= LatinPunktSentenceTokenizer()\n",
    "nltk_sents = sent_tokenizer.tokenize(test)\n",
    "\n",
    "for i, sent in enumerate(nltk_sents, 1):\n",
    "    print(f'{i}: {sent}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rostock-workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "271717606410ac57bb3ccde2581fc7e1f6e46ce9d2d102e43e700f6187669d78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
